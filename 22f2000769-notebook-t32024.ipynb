{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465620c5",
   "metadata": {
    "papermill": {
     "duration": 0.012397,
     "end_time": "2024-12-11T08:51:31.743976",
     "exception": false,
     "start_time": "2024-12-11T08:51:31.731579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 35px; font-family: 'Montserrat', sans-serif; font-weight: 900; color: #7D7D7D; text-align: center; text-transform: uppercase; letter-spacing: 2px; background: linear-gradient(to right, #D3D3D3, #A9A9A9); color: white; padding: 15px 30px; border-radius: 25px; box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\">\n",
    "    Predicting the success of Bank telemarketing </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa750b0",
   "metadata": {
    "papermill": {
     "duration": 0.005933,
     "end_time": "2024-12-11T08:51:31.756373",
     "exception": false,
     "start_time": "2024-12-11T08:51:31.750440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Important Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e227cdf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:31.770556Z",
     "iopub.status.busy": "2024-12-11T08:51:31.770031Z",
     "iopub.status.idle": "2024-12-11T08:51:34.246305Z",
     "shell.execute_reply": "2024-12-11T08:51:34.245363Z"
    },
    "papermill": {
     "duration": 2.486219,
     "end_time": "2024-12-11T08:51:34.248698",
     "exception": false,
     "start_time": "2024-12-11T08:51:31.762479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f9daa8",
   "metadata": {
    "papermill": {
     "duration": 0.005775,
     "end_time": "2024-12-11T08:51:34.260741",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.254966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Libraries for Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89068728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:34.275556Z",
     "iopub.status.busy": "2024-12-11T08:51:34.274966Z",
     "iopub.status.idle": "2024-12-11T08:51:34.534362Z",
     "shell.execute_reply": "2024-12-11T08:51:34.533087Z"
    },
    "papermill": {
     "duration": 0.269184,
     "end_time": "2024-12-11T08:51:34.536853",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.267669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa86939",
   "metadata": {
    "papermill": {
     "duration": 0.0058,
     "end_time": "2024-12-11T08:51:34.549095",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.543295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f813e226",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:34.564474Z",
     "iopub.status.busy": "2024-12-11T08:51:34.563596Z",
     "iopub.status.idle": "2024-12-11T08:51:34.741095Z",
     "shell.execute_reply": "2024-12-11T08:51:34.740230Z"
    },
    "papermill": {
     "duration": 0.187884,
     "end_time": "2024-12-11T08:51:34.743393",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.555509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf15e16",
   "metadata": {
    "papermill": {
     "duration": 0.005789,
     "end_time": "2024-12-11T08:51:34.755338",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.749549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059ecb83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:34.769124Z",
     "iopub.status.busy": "2024-12-11T08:51:34.768704Z",
     "iopub.status.idle": "2024-12-11T08:51:34.866030Z",
     "shell.execute_reply": "2024-12-11T08:51:34.864922Z"
    },
    "papermill": {
     "duration": 0.107095,
     "end_time": "2024-12-11T08:51:34.868546",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.761451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35661e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:34.882752Z",
     "iopub.status.busy": "2024-12-11T08:51:34.882346Z",
     "iopub.status.idle": "2024-12-11T08:51:35.057683Z",
     "shell.execute_reply": "2024-12-11T08:51:35.056519Z"
    },
    "papermill": {
     "duration": 0.185155,
     "end_time": "2024-12-11T08:51:35.059959",
     "exception": false,
     "start_time": "2024-12-11T08:51:34.874804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last contact date</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-17</td>\n",
       "      <td>26</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>647</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>52</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>553</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-11-20</td>\n",
       "      <td>44</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1397</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>33</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>394</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-29</td>\n",
       "      <td>31</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>137</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>445</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2008-05-15</td>\n",
       "      <td>34</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>-17</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2009-05-11</td>\n",
       "      <td>57</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>124</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>296</td>\n",
       "      <td>1</td>\n",
       "      <td>287</td>\n",
       "      <td>12</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2008-05-07</td>\n",
       "      <td>39</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-219</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2010-07-22</td>\n",
       "      <td>53</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2009-02-03</td>\n",
       "      <td>34</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>123</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   last contact date  age           job  marital  education default  balance  \\\n",
       "0         2009-04-17   26   blue-collar  married  secondary      no      647   \n",
       "1         2009-10-11   52    technician  married  secondary      no      553   \n",
       "2         2010-11-20   44   blue-collar  married  secondary      no     1397   \n",
       "3         2009-09-01   33        admin.  married  secondary      no      394   \n",
       "4         2008-01-29   31  entrepreneur   single   tertiary      no      137   \n",
       "..               ...  ...           ...      ...        ...     ...      ...   \n",
       "95        2008-05-15   34   blue-collar  married        NaN      no      -17   \n",
       "96        2009-05-11   57    technician  married  secondary      no      124   \n",
       "97        2008-05-07   39   blue-collar  married  secondary      no     -219   \n",
       "98        2010-07-22   53  entrepreneur  married  secondary      no        0   \n",
       "99        2009-02-03   34    management  married   tertiary      no      123   \n",
       "\n",
       "   housing loan    contact  duration  campaign  pdays  previous poutcome  \\\n",
       "0      yes   no   cellular       357         2    331         1    other   \n",
       "1      yes   no  telephone       160         1     -1         0      NaN   \n",
       "2       no   no   cellular       326         1     -1         0      NaN   \n",
       "3      yes   no  telephone       104         3     -1         0      NaN   \n",
       "4       no   no   cellular       445         2     -1         0      NaN   \n",
       "..     ...  ...        ...       ...       ...    ...       ...      ...   \n",
       "95     yes   no        NaN       319         7     -1         0      NaN   \n",
       "96     yes   no  telephone       296         1    287        12  failure   \n",
       "97     yes   no   cellular       101         1     -1         0      NaN   \n",
       "98     yes   no   cellular       200         1     -1         0      NaN   \n",
       "99      no   no   cellular       301         3     -1         0      NaN   \n",
       "\n",
       "   target  \n",
       "0      no  \n",
       "1      no  \n",
       "2      no  \n",
       "3      no  \n",
       "4      no  \n",
       "..    ...  \n",
       "95     no  \n",
       "96     no  \n",
       "97     no  \n",
       "98     no  \n",
       "99    yes  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dataset from the provided file path and store it in the df dataframe\n",
    "df = pd.read_csv('/kaggle/input/predict-the-success-of-bank-telemarketing/train.csv')\n",
    "\n",
    "# Display the first 100 rows of the loaded training dataframe to inspect its structure\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270ffe6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.075262Z",
     "iopub.status.busy": "2024-12-11T08:51:35.074823Z",
     "iopub.status.idle": "2024-12-11T08:51:35.099905Z",
     "shell.execute_reply": "2024-12-11T08:51:35.098873Z"
    },
    "papermill": {
     "duration": 0.035533,
     "end_time": "2024-12-11T08:51:35.102321",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.066788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last contact date</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-04-17</td>\n",
       "      <td>26</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>647</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>331</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-10-11</td>\n",
       "      <td>52</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>553</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-11-20</td>\n",
       "      <td>44</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1397</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>326</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-01</td>\n",
       "      <td>33</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>394</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-01-29</td>\n",
       "      <td>31</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>137</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>445</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last contact date  age           job  marital  education default  balance  \\\n",
       "0        2009-04-17   26   blue-collar  married  secondary      no      647   \n",
       "1        2009-10-11   52    technician  married  secondary      no      553   \n",
       "2        2010-11-20   44   blue-collar  married  secondary      no     1397   \n",
       "3        2009-09-01   33        admin.  married  secondary      no      394   \n",
       "4        2008-01-29   31  entrepreneur   single   tertiary      no      137   \n",
       "\n",
       "  housing loan    contact  duration  campaign  pdays  previous target  \n",
       "0     yes   no   cellular       357         2    331         1     no  \n",
       "1     yes   no  telephone       160         1     -1         0     no  \n",
       "2      no   no   cellular       326         1     -1         0     no  \n",
       "3     yes   no  telephone       104         3     -1         0     no  \n",
       "4      no   no   cellular       445         2     -1         0     no  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'poutcome' column from the dataframe (df) as it might not be relevant or necessary for the analysis\n",
    "df = df.drop('poutcome', axis=1)\n",
    "\n",
    "# Display the first few rows of the dataframe after dropping the 'poutcome' column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdfba110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.118219Z",
     "iopub.status.busy": "2024-12-11T08:51:35.117763Z",
     "iopub.status.idle": "2024-12-11T08:51:35.170227Z",
     "shell.execute_reply": "2024-12-11T08:51:35.169028Z"
    },
    "papermill": {
     "duration": 0.062896,
     "end_time": "2024-12-11T08:51:35.172411",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.109515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last contact date</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-21</td>\n",
       "      <td>36</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>7</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1067</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-07-28</td>\n",
       "      <td>32</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>82</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-06-09</td>\n",
       "      <td>38</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1487</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>332</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-03-02</td>\n",
       "      <td>59</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>315</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>2</td>\n",
       "      <td>failure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last contact date  age          job  marital  education default  balance  \\\n",
       "0        2009-11-21   36   management   single   tertiary      no        7   \n",
       "1        2010-02-04   30   unemployed  married   tertiary      no     1067   \n",
       "2        2010-07-28   32  blue-collar   single  secondary      no       82   \n",
       "3        2010-06-09   38       admin.  married    primary      no     1487   \n",
       "4        2008-03-02   59   management  married   tertiary      no      315   \n",
       "\n",
       "  housing loan   contact  duration  campaign  pdays  previous poutcome  \n",
       "0      no   no       NaN        20         1     -1         0      NaN  \n",
       "1      no   no  cellular        78         2     -1         0      NaN  \n",
       "2     yes   no  cellular        86         4     -1         0      NaN  \n",
       "3      no   no       NaN       332         2     -1         0      NaN  \n",
       "4      no   no  cellular       591         1    176         2  failure  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test dataset from the provided file path and store it in the df_test dataframe\n",
    "df_test = pd.read_csv(\"/kaggle/input/predict-the-success-of-bank-telemarketing/test.csv\")\n",
    "\n",
    "# Display the first few rows of the loaded test dataframe to inspect its structure\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cac922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.188680Z",
     "iopub.status.busy": "2024-12-11T08:51:35.188277Z",
     "iopub.status.idle": "2024-12-11T08:51:35.204387Z",
     "shell.execute_reply": "2024-12-11T08:51:35.203289Z"
    },
    "papermill": {
     "duration": 0.027039,
     "end_time": "2024-12-11T08:51:35.206702",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.179663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last contact date</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-11-21</td>\n",
       "      <td>36</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>7</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1067</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-07-28</td>\n",
       "      <td>32</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>82</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-06-09</td>\n",
       "      <td>38</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1487</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>332</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-03-02</td>\n",
       "      <td>59</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>315</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>591</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last contact date  age          job  marital  education default  balance  \\\n",
       "0        2009-11-21   36   management   single   tertiary      no        7   \n",
       "1        2010-02-04   30   unemployed  married   tertiary      no     1067   \n",
       "2        2010-07-28   32  blue-collar   single  secondary      no       82   \n",
       "3        2010-06-09   38       admin.  married    primary      no     1487   \n",
       "4        2008-03-02   59   management  married   tertiary      no      315   \n",
       "\n",
       "  housing loan   contact  duration  campaign  pdays  previous  \n",
       "0      no   no       NaN        20         1     -1         0  \n",
       "1      no   no  cellular        78         2     -1         0  \n",
       "2     yes   no  cellular        86         4     -1         0  \n",
       "3      no   no       NaN       332         2     -1         0  \n",
       "4      no   no  cellular       591         1    176         2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'poutcome' column from the test dataframe df_test\n",
    "df_test = df_test.drop('poutcome', axis=1)\n",
    "\n",
    "# Display the first few rows of the updated df_test dataframe\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ecb85c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.223773Z",
     "iopub.status.busy": "2024-12-11T08:51:35.223360Z",
     "iopub.status.idle": "2024-12-11T08:51:35.233395Z",
     "shell.execute_reply": "2024-12-11T08:51:35.232272Z"
    },
    "papermill": {
     "duration": 0.020855,
     "end_time": "2024-12-11T08:51:35.235557",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.214702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the 'target' column from the dataframe and assign it to the variable 'x' (features)\n",
    "x = df.drop('target', axis=1)\n",
    "\n",
    "# Assign the 'target' column to the variable 'y' (target variable)\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f521757a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.252576Z",
     "iopub.status.busy": "2024-12-11T08:51:35.252180Z",
     "iopub.status.idle": "2024-12-11T08:51:35.288583Z",
     "shell.execute_reply": "2024-12-11T08:51:35.287611Z"
    },
    "papermill": {
     "duration": 0.047845,
     "end_time": "2024-12-11T08:51:35.291085",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.243240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the 'last contact date' column to datetime format\n",
    "x['last contact date'] = pd.to_datetime(x['last contact date'])\n",
    "\n",
    "# Extract the year from the 'last contact date' and create a new column 'contact_year'\n",
    "x['contact_year'] = x['last contact date'].dt.year\n",
    "\n",
    "# Extract the month from the 'last contact date' and create a new column 'contact_month'\n",
    "x['contact_month'] = x['last contact date'].dt.month\n",
    "\n",
    "# Extract the day from the 'last contact date' and create a new column 'contact_day'\n",
    "x['contact_day'] = x['last contact date'].dt.day\n",
    "\n",
    "# Drop the original 'last contact date' column as we no longer need it\n",
    "x = x.drop('last contact date', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9614368e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.308418Z",
     "iopub.status.busy": "2024-12-11T08:51:35.308002Z",
     "iopub.status.idle": "2024-12-11T08:51:35.506858Z",
     "shell.execute_reply": "2024-12-11T08:51:35.505614Z"
    },
    "papermill": {
     "duration": 0.210253,
     "end_time": "2024-12-11T08:51:35.509133",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.298880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31368, 26)\n",
      "(7843, 26)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define numerical and categorical columns\n",
    "num_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous', 'contact_year', 'contact_month', 'contact_day']\n",
    "cat_cols = ['job', 'marital', 'default', 'housing', 'loan', 'contact']\n",
    "edu_col = ['education']  # Education column\n",
    "\n",
    "# Step 3: Create preprocessing pipelines\n",
    "# For numerical columns\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean of the column\n",
    "    ('scaler', StandardScaler())  # Scale features (standardize) to mean=0, std=1 for better model performance\n",
    "])\n",
    "\n",
    "# For categorical columns (excluding education)\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent category\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))  # One-hot encoding (avoid dummy variable trap)\n",
    "])\n",
    "\n",
    "# For education column using Ordinal Encoding (specific ranking between categories)\n",
    "edu_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent category\n",
    "    ('ordinal', OrdinalEncoder(categories=[['primary', 'secondary', 'tertiary']]))  # Ordinal encoding for education (primary < secondary < tertiary)\n",
    "])\n",
    "\n",
    "# Step 4: Combine all pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, num_cols),  # Apply num_pipeline to num_cols (numerical columns)\n",
    "        ('cat', cat_pipeline, cat_cols),   # Apply cat_pipeline to cat_cols (categorical columns)\n",
    "        ('edu', edu_pipeline, edu_col)     # Apply edu_pipeline to education column\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Create a full pipeline with the preprocessor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)  # Preprocessing step applied to all features\n",
    "])\n",
    "\n",
    "# Step 7: Fit the pipeline on the training data\n",
    "# Fit the preprocessor and transform the training set\n",
    "x_train_transformed = pipeline.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data using the same preprocessing steps\n",
    "x_test_transformed = pipeline.transform(x_test)\n",
    "\n",
    "\n",
    "# Convert the transformed NumPy arrays back to DataFrames with proper column names\n",
    "x_train_transformed = pd.DataFrame(x_train_transformed, columns=preprocessor.get_feature_names_out())\n",
    "x_test_transformed = pd.DataFrame(x_test_transformed, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Output the shape of the transformed training and test sets to ensure they are correctly processed\n",
    "print(x_train_transformed.shape)\n",
    "print(x_test_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3c9ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.526029Z",
     "iopub.status.busy": "2024-12-11T08:51:35.525686Z",
     "iopub.status.idle": "2024-12-11T08:51:35.530424Z",
     "shell.execute_reply": "2024-12-11T08:51:35.529361Z"
    },
    "papermill": {
     "duration": 0.015617,
     "end_time": "2024-12-11T08:51:35.532442",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.516825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = df_test # Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f00127b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.549597Z",
     "iopub.status.busy": "2024-12-11T08:51:35.549213Z",
     "iopub.status.idle": "2024-12-11T08:51:35.564302Z",
     "shell.execute_reply": "2024-12-11T08:51:35.563002Z"
    },
    "papermill": {
     "duration": 0.026491,
     "end_time": "2024-12-11T08:51:35.566919",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.540428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'last contact date' column to datetime format\n",
    "z['last contact date'] = pd.to_datetime(z['last contact date'])\n",
    "\n",
    "# Extract the year from the 'last contact date' and store it in a new column 'contact_year'\n",
    "z['contact_year'] = z['last contact date'].dt.year\n",
    "\n",
    "# Extract the month from the 'last contact date' and store it in a new column 'contact_month'\n",
    "z['contact_month'] = z['last contact date'].dt.month\n",
    "\n",
    "# Extract the day from the 'last contact date' and store it in a new column 'contact_day'\n",
    "z['contact_day'] = z['last contact date'].dt.day\n",
    "\n",
    "# Drop the 'last contact date' column from the dataframe, as it's no longer needed\n",
    "z = z.drop('last contact date', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af46922a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.584732Z",
     "iopub.status.busy": "2024-12-11T08:51:35.584324Z",
     "iopub.status.idle": "2024-12-11T08:51:35.645646Z",
     "shell.execute_reply": "2024-12-11T08:51:35.644489Z"
    },
    "papermill": {
     "duration": 0.072976,
     "end_time": "2024-12-11T08:51:35.647957",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.574981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__age</th>\n",
       "      <th>num__balance</th>\n",
       "      <th>num__duration</th>\n",
       "      <th>num__campaign</th>\n",
       "      <th>num__pdays</th>\n",
       "      <th>num__previous</th>\n",
       "      <th>num__contact_year</th>\n",
       "      <th>num__contact_month</th>\n",
       "      <th>num__contact_day</th>\n",
       "      <th>cat__job_blue-collar</th>\n",
       "      <th>...</th>\n",
       "      <th>cat__job_student</th>\n",
       "      <th>cat__job_technician</th>\n",
       "      <th>cat__job_unemployed</th>\n",
       "      <th>cat__marital_married</th>\n",
       "      <th>cat__marital_single</th>\n",
       "      <th>cat__default_yes</th>\n",
       "      <th>cat__housing_yes</th>\n",
       "      <th>cat__loan_yes</th>\n",
       "      <th>cat__contact_telephone</th>\n",
       "      <th>edu__education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.483611</td>\n",
       "      <td>-0.331969</td>\n",
       "      <td>-0.544390</td>\n",
       "      <td>-0.414670</td>\n",
       "      <td>-0.454847</td>\n",
       "      <td>-0.266955</td>\n",
       "      <td>-0.000858</td>\n",
       "      <td>1.827486</td>\n",
       "      <td>0.626869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.955588</td>\n",
       "      <td>-0.266765</td>\n",
       "      <td>-0.468921</td>\n",
       "      <td>-0.313649</td>\n",
       "      <td>-0.454847</td>\n",
       "      <td>-0.266955</td>\n",
       "      <td>1.222368</td>\n",
       "      <td>-1.620194</td>\n",
       "      <td>-1.408964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.798262</td>\n",
       "      <td>-0.327356</td>\n",
       "      <td>-0.458512</td>\n",
       "      <td>-0.111609</td>\n",
       "      <td>-0.454847</td>\n",
       "      <td>-0.266955</td>\n",
       "      <td>1.222368</td>\n",
       "      <td>0.295184</td>\n",
       "      <td>1.465154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.326285</td>\n",
       "      <td>-0.240929</td>\n",
       "      <td>-0.138422</td>\n",
       "      <td>-0.313649</td>\n",
       "      <td>-0.454847</td>\n",
       "      <td>-0.266955</td>\n",
       "      <td>1.222368</td>\n",
       "      <td>-0.087892</td>\n",
       "      <td>-0.810190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.325635</td>\n",
       "      <td>-0.313023</td>\n",
       "      <td>0.198583</td>\n",
       "      <td>-0.414670</td>\n",
       "      <td>0.641016</td>\n",
       "      <td>-0.221716</td>\n",
       "      <td>-1.224083</td>\n",
       "      <td>-1.237119</td>\n",
       "      <td>-1.648474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num__age  num__balance  num__duration  num__campaign  num__pdays  \\\n",
       "0 -0.483611     -0.331969      -0.544390      -0.414670   -0.454847   \n",
       "1 -0.955588     -0.266765      -0.468921      -0.313649   -0.454847   \n",
       "2 -0.798262     -0.327356      -0.458512      -0.111609   -0.454847   \n",
       "3 -0.326285     -0.240929      -0.138422      -0.313649   -0.454847   \n",
       "4  1.325635     -0.313023       0.198583      -0.414670    0.641016   \n",
       "\n",
       "   num__previous  num__contact_year  num__contact_month  num__contact_day  \\\n",
       "0      -0.266955          -0.000858            1.827486          0.626869   \n",
       "1      -0.266955           1.222368           -1.620194         -1.408964   \n",
       "2      -0.266955           1.222368            0.295184          1.465154   \n",
       "3      -0.266955           1.222368           -0.087892         -0.810190   \n",
       "4      -0.221716          -1.224083           -1.237119         -1.648474   \n",
       "\n",
       "   cat__job_blue-collar  ...  cat__job_student  cat__job_technician  \\\n",
       "0                   0.0  ...               0.0                  0.0   \n",
       "1                   0.0  ...               0.0                  0.0   \n",
       "2                   1.0  ...               0.0                  0.0   \n",
       "3                   0.0  ...               0.0                  0.0   \n",
       "4                   0.0  ...               0.0                  0.0   \n",
       "\n",
       "   cat__job_unemployed  cat__marital_married  cat__marital_single  \\\n",
       "0                  0.0                   0.0                  1.0   \n",
       "1                  1.0                   1.0                  0.0   \n",
       "2                  0.0                   0.0                  1.0   \n",
       "3                  0.0                   1.0                  0.0   \n",
       "4                  0.0                   1.0                  0.0   \n",
       "\n",
       "   cat__default_yes  cat__housing_yes  cat__loan_yes  cat__contact_telephone  \\\n",
       "0               0.0               0.0            0.0                     0.0   \n",
       "1               0.0               0.0            0.0                     0.0   \n",
       "2               0.0               1.0            0.0                     0.0   \n",
       "3               0.0               0.0            0.0                     0.0   \n",
       "4               0.0               0.0            0.0                     0.0   \n",
       "\n",
       "   edu__education  \n",
       "0             2.0  \n",
       "1             2.0  \n",
       "2             1.0  \n",
       "3             0.0  \n",
       "4             2.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the validation or test set using the previously fitted pipeline\n",
    "z_test_transformed = pipeline.transform(z)\n",
    "\n",
    "# Convert the transformed data into a DataFrame and set the correct column names based on the feature names from the preprocessor\n",
    "z_test_transformed = pd.DataFrame(z_test_transformed, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the transformed test data to check the result\n",
    "z_test_transformed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a4324",
   "metadata": {
    "papermill": {
     "duration": 0.007743,
     "end_time": "2024-12-11T08:51:35.663971",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.656228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Confusion Matrix for \"Defaulter\" and \"Non-Defaulter\"\r\n",
    "\r\n",
    "|                         | **Defaulter (Predicted: Yes)** | **Non-Defaulter (Predicted: No)** |\r\n",
    "|-------------------------|--------------------------------|----------------------------------|\r\n",
    "| **Defaulter (Actual: Yes)** | True Positive (TP): Correctly predicted as defaulter | False Negative (FN): Incorrectly predicted as non-defaulter |\r\n",
    "| **Non-Defaulter (Actual: No)** | False Positive (FP): Incorrectly predicted as defaulter | True Negative (TN): Correctly predicted as non-defaulter |\r\n",
    "\r\n",
    "- # Simple Explanation:\r\n",
    "\r\n",
    "- **True Positive (TP)**: The model correctly predicts someone as a defaulter (they are a defaulter).\r\n",
    "- **False Positive (FP)**: The model wrongly predicts someone as a defaulter when they are actually not (this is bad for the customer).\r\n",
    "- **False Negative (FN)**: The model wrongly predicts someone as not a defaulter when they actually are (this is bad for the bank).\r\n",
    "- **True Negative (TN)**: The model correctly predicts someone as not a defaulter (they are not a de\n",
    "---faulter).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Why is this important?\r\n",
    "\r\n",
    "- **False Positives (FP)** affect **Precision**:  \r\n",
    "  If the model predicts a customer as a defaulter when they are actually not (False Positive), it will **lower the Precision**. This means that the bank is wrongly identifying people who are **not defaulters** as defaulters. This could cause harm to customers, affecting their credit ratings and causing unnecessary actions like blocking loans.\r\n",
    "\r\n",
    "- **False Negatives (FN)** affect **Recall**:  \r\n",
    "  If the model predicts a customer as **not a defaulter** when they are actually a defaulter (False Negative), it will **lower Recall**. This means that the bank is missing out on identifying people who are actual defaulters. This could result in financial loss for the bank, as defaulters would not be caught and might not pay back their debts.\r\n",
    "\r\n",
    "In essence, we need to balance **Precision** and **Recall** to ensure both the **customer's safety** (no false accusations) and the **bank's financial security** (no missed defaulters). \r\n",
    "\r\n",
    "- **Precision** is important for ensuring that the bank doesn't wrongly flag people as defaulters.  \r\n",
    "- **Recall** is important for ensuring that the bank doesn't miss actual defaulters.  \r\n",
    "\r\n",
    "The **F1 Score** is a great way to balance **Precision** and **Recall**. By focusing on improving both, we can make the model more reliable and fair for both the **bank** and the **customers**.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bd758",
   "metadata": {
    "papermill": {
     "duration": 0.00766,
     "end_time": "2024-12-11T08:51:35.679687",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.672027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 42px; font-family: 'Montserrat', sans-serif; font-weight: 900; color: #7D7D7D; text-align: center; text-transform: uppercase; letter-spacing: 2px; background: linear-gradient(to right, #D3D3D3, #A9A9A9); color: white; padding: 15px 30px; border-radius: 25px; box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\">\n",
    "    Model 1 - Logistic Regression </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2277cf",
   "metadata": {
    "papermill": {
     "duration": 0.007666,
     "end_time": "2024-12-11T08:51:35.695278",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.687612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why Logistic Regression?\r\n",
    "\r\n",
    "1. **Simplicity and Interpretability**: Logistic Regression is a simple, linear model that's easy to understand and interpret. It's useful in business scenarios where understanding the reasons behind predictions is important.\r\n",
    "\r\n",
    "2. **Probabilistic Output**: It provides probabilities, making it flexible in decision-making. You can set a threshold to determine when to flag a customer as a defaulter, such as when the model's confidence is above 70%.\r\n",
    "\r\n",
    "3. **Efficient for Binary Classification**: It's ideal for binary classification tasks like predicting whether a customer is a defaulter or not (Yes/No).\r\n",
    "\r\n",
    "4. **Less Prone to Overfitting**: Compared to more complex models, Logistic Regression is less prone to overfitting, especially when the dataset is smaller.\r\n",
    "\r\n",
    "5. **Computationally Efficient**: It is less computationally expensive and can handle large datasets efficiently, which is essential for real-time predictions in business environments.\r\n",
    "\r\n",
    "6. **Good Baseline Model**: Logistic Regression provides solid performance and is often used as a baseline model before experimenting with more complex aclassification tasks.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53fd821c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:51:35.713196Z",
     "iopub.status.busy": "2024-12-11T08:51:35.712761Z",
     "iopub.status.idle": "2024-12-11T08:52:16.464998Z",
     "shell.execute_reply": "2024-12-11T08:52:16.461178Z"
    },
    "papermill": {
     "duration": 40.766746,
     "end_time": "2024-12-11T08:52:16.469895",
     "exception": false,
     "start_time": "2024-12-11T08:51:35.703149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters for Logistic Regression: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 100, 'C': 0.01}\n",
      "Logistic Regression Training Accuracy: 0.8528755419535833\n",
      "Logistic Regression Test Accuracy: 0.8489098559224787\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          no       0.86      0.98      0.92      6645\n",
      "         yes       0.52      0.15      0.23      1198\n",
      "\n",
      "    accuracy                           0.85      7843\n",
      "   macro avg       0.69      0.56      0.57      7843\n",
      "weighted avg       0.81      0.85      0.81      7843\n",
      "\n",
      "Training F1 Score (Macro): 0.57\n",
      "Test F1 Score (Macro): 0.57\n",
      "Training ROC AUC Score: 0.80\n",
      "Test ROC AUC Score: 0.79\n",
      "Submission file created successfully: 'submission_log_reg.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model with a random seed for reproducibility\n",
    "log_reg_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Defining a dictionary with the hyperparameters to tune during RandomizedSearchCV\n",
    "param_dist_log_reg = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # This controls the regularization strength\n",
    "    'penalty': ['l2', 'l1'],  # Type of regularization (either L1 or L2)\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers to use for optimization\n",
    "    'max_iter': [100, 200, 500]  # Maximum number of iterations for convergence\n",
    "}\n",
    "\n",
    "# Setting up RandomizedSearchCV to perform hyperparameter tuning\n",
    "random_search_log_reg = RandomizedSearchCV(log_reg_model, \n",
    "                                           param_distributions=param_dist_log_reg, \n",
    "                                           n_iter=50,  # Number of different hyperparameter combinations to try\n",
    "                                           scoring='f1_macro',  # Optimizing the F1 score (macro average)\n",
    "                                           cv=5,  # 5-fold cross-validation to evaluate performance\n",
    "                                           verbose=2,  # Show detailed output for progress tracking\n",
    "                                           random_state=42,  # Ensures reproducibility of results\n",
    "                                           n_jobs=-1)  # Use all available cores for faster processing\n",
    "\n",
    "# Fitting the RandomizedSearchCV to the training data\n",
    "random_search_log_reg.fit(x_train_transformed, y_train)\n",
    "\n",
    "# Get the best Logistic Regression model after hyperparameter tuning\n",
    "log_reg_model = random_search_log_reg.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters found by RandomizedSearchCV\n",
    "print(\"Best Parameters for Logistic Regression:\", random_search_log_reg.best_params_)\n",
    "\n",
    "# Making predictions for the test and train sets\n",
    "y_test_pred_log_reg = log_reg_model.predict(x_test_transformed)\n",
    "y_train_pred_log_reg = log_reg_model.predict(x_train_transformed)\n",
    "\n",
    "# Evaluating the model by calculating the accuracy and printing the classification report\n",
    "print(\"Logistic Regression Training Accuracy:\", accuracy_score(y_train, y_train_pred_log_reg))\n",
    "print(\"Logistic Regression Test Accuracy:\", accuracy_score(y_test, y_test_pred_log_reg))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_test_pred_log_reg))\n",
    "\n",
    "# Calculate and print F1 Score (macro average) for both train and test sets\n",
    "f1_train = f1_score(y_train, y_train_pred_log_reg, average='macro')\n",
    "f1_test = f1_score(y_test, y_test_pred_log_reg, average='macro')\n",
    "print(f\"Training F1 Score (Macro): {f1_train:.2f}\")\n",
    "print(f\"Test F1 Score (Macro): {f1_test:.2f}\")\n",
    "\n",
    "# Calculate and print ROC AUC Score for both train and test sets\n",
    "roc_auc_train = roc_auc_score(y_train, log_reg_model.predict_proba(x_train_transformed)[:, 1])\n",
    "roc_auc_test = roc_auc_score(y_test, log_reg_model.predict_proba(x_test_transformed)[:, 1])\n",
    "print(f\"Training ROC AUC Score: {roc_auc_train:.2f}\")\n",
    "print(f\"Test ROC AUC Score: {roc_auc_test:.2f}\")\n",
    "\n",
    "# --- Now, let's create the submission file ---\n",
    "# Predicting the target variable for the test set (for submission purposes)\n",
    "y_test_pred_submission_log_reg = log_reg_model.predict(z_test_transformed)  # Using the test set (z_test_transformed)\n",
    "\n",
    "# Creating a DataFrame to store the ID and predictions (required for submission)\n",
    "submission_log_reg = pd.DataFrame({\n",
    "    'id': range(0, len(z_test_transformed)),  # Creating an ID column (from 0 to the length of the test data)\n",
    "    'target': y_test_pred_submission_log_reg  # Storing the predicted target values\n",
    "})\n",
    "\n",
    "# Saving the DataFrame to a CSV file named 'submission_log_reg.csv'\n",
    "submission_log_reg.to_csv('submission_log_reg.csv', index=False)\n",
    "\n",
    "# Informing that the submission file has been successfully created\n",
    "print(\"Submission file created successfully: 'submission_log_reg.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073c019",
   "metadata": {
    "papermill": {
     "duration": 0.023454,
     "end_time": "2024-12-11T08:52:16.517860",
     "exception": false,
     "start_time": "2024-12-11T08:52:16.494406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**When is Logistic Regression not suitable?(Cons)**\n",
    "- When the data is highly **non-linear**, more advanced models like **Random Forests** may perform better.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb4840",
   "metadata": {
    "papermill": {
     "duration": 0.013872,
     "end_time": "2024-12-11T08:52:16.557041",
     "exception": false,
     "start_time": "2024-12-11T08:52:16.543169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<p style=\"font-size: 42px; font-family: 'Montserrat', sans-serif; font-weight: 900; color: #7D7D7D; text-align: center; text-transform: uppercase; letter-spacing: 2px; background: linear-gradient(to right, #D3D3D3, #A9A9A9); color: white; padding: 15px 30px; border-radius: 25px; box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\">\n",
    "    Model 2 - SGD Classifier </p>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bff78",
   "metadata": {
    "papermill": {
     "duration": 0.008395,
     "end_time": "2024-12-11T08:52:16.574231",
     "exception": false,
     "start_time": "2024-12-11T08:52:16.565836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why SGD Classifier?\r\n",
    "\r\n",
    "- **Efficiency with Large Datasets**: SGD is fast and efficient, especially with large datasets. It updates weights incrementally, making it suitable for large-scale classification problems.\r\n",
    "  \r\n",
    "- **Flexibility**: SGD works with different loss functions and penalties, making it a versatile model for a wide range of tasks, such as logistic regression and SVM.\r\n",
    "\r\n",
    "- **Memory Efficient**: Since it processes one sample at a time, SGD is memory efficient, which is beneficial when dealing with large datasets.\r\n",
    "\r\n",
    "- **Good for Sparse Data**: SGD performs well with sparse data, such as text classification, where many features are zeroistic regression.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4252638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:52:16.593907Z",
     "iopub.status.busy": "2024-12-11T08:52:16.592912Z",
     "iopub.status.idle": "2024-12-11T08:52:26.518345Z",
     "shell.execute_reply": "2024-12-11T08:52:26.514528Z"
    },
    "papermill": {
     "duration": 9.939994,
     "end_time": "2024-12-11T08:52:26.522744",
     "exception": false,
     "start_time": "2024-12-11T08:52:16.582750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.0001, 'eta0': 1.0, 'learning_rate': 'adaptive', 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1', 'tol': 0.0001}\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.82      0.88      6645\n",
      "         yes       0.42      0.73      0.53      1198\n",
      "\n",
      "    accuracy                           0.80      7843\n",
      "   macro avg       0.68      0.77      0.70      7843\n",
      "weighted avg       0.86      0.80      0.82      7843\n",
      "\n",
      "Confusion Matrix:\n",
      " [[5426 1219]\n",
      " [ 328  870]]\n",
      "Test F1 Score: 0.82\n",
      "Test ROC-AUC Score: 0.84\n",
      "Submission file saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Resample the training set using SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "x_resampled, y_resampled = smote.fit_resample(x_train_transformed, y_train)\n",
    "\n",
    "# Define the SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter grid with added 'tol' for tolerance\n",
    "param_grid = {\n",
    "    'loss': ['hinge'],  # 'hinge' loss corresponds to linear SVM\n",
    "    'penalty': ['l1'],  # L1 regularization (Lasso)\n",
    "    'alpha': [0.0001],  # Regularization strength\n",
    "    'learning_rate': ['adaptive'],  # Adaptive learning rate\n",
    "    'eta0': [1.0],  # Initial learning rate\n",
    "    'max_iter': [1000],  # Number of iterations\n",
    "    'tol': [1e-4, 1e-3, 1e-2]  # Tolerance values for early stopping\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=sgd,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',  # Use weighted F1-score to handle class imbalance\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=0,  # Set verbose to 1 to see the progress of the grid search\n",
    "    n_jobs=-1  # Use all cores to speed up the computation\n",
    ")\n",
    "\n",
    "# Fit the model with resampled data\n",
    "grid_search.fit(x_resampled, y_resampled)\n",
    "\n",
    "# Best model after grid search\n",
    "best_sgd = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = best_sgd.predict(x_test_transformed)\n",
    "y_test_probs = best_sgd.decision_function(x_test_transformed)\n",
    "\n",
    "# Evaluate model performance\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_probs)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Display F1 Score and ROC-AUC score\n",
    "print(f\"Test F1 Score: {test_f1:.2f}\")\n",
    "print(f\"Test ROC-AUC Score: {test_roc_auc:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test data for submission\n",
    "z_test_pred = best_sgd.predict(z_test_transformed)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(z_test_pred)),  # Replace with actual ID column if available\n",
    "    'target': z_test_pred\n",
    "})\n",
    "\n",
    "# Save submission to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file saved as 'submission.csv'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65492dd",
   "metadata": {
    "papermill": {
     "duration": 0.024577,
     "end_time": "2024-12-11T08:52:26.602382",
     "exception": false,
     "start_time": "2024-12-11T08:52:26.577805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# Why not SGD?\n",
    "\n",
    "- **Sensitive to Hyperparameters**: SGD is sensitive to the learning rate and requires careful tuning. If not set correctly, it may lead to poor results.\n",
    "\n",
    "- **Convergence Issues**: The algorithm may not converge if the learning rate is too high or too low, leading to oscillations or slow progress.\n",
    "\n",
    "- **Requires Proper Scaling**: Feature scaling (like normalization or standardization) is essential for proper convergence of SGD, especially in models like logistic regression.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8cc60",
   "metadata": {
    "papermill": {
     "duration": 0.008164,
     "end_time": "2024-12-11T08:52:26.620760",
     "exception": false,
     "start_time": "2024-12-11T08:52:26.612596",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<p style=\"font-size: 42px; font-family: 'Montserrat', sans-serif; font-weight: 900; color: #7D7D7D; text-align: center; text-transform: uppercase; letter-spacing: 2px; background: linear-gradient(to right, #D3D3D3, #A9A9A9); color: white; padding: 15px 30px; border-radius: 25px; box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\">\n",
    "    Model 3 - Random Forest </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbb955",
   "metadata": {
    "papermill": {
     "duration": 0.008219,
     "end_time": "2024-12-11T08:52:26.637560",
     "exception": false,
     "start_time": "2024-12-11T08:52:26.629341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why Random Forest?\r\n",
    "\r\n",
    "- **Robust to Overfitting**: Random Forest is an ensemble method that reduces the risk of overfitting by averaging multiple decision trees, making it more robust compared to a single decision tree.\r\n",
    "\r\n",
    "- **Handles Non-linear Data**: It can capture complex, non-linear relationships in data, making it a great choice for a variety of tasks.\r\n",
    "\r\n",
    "- **Handles Missing Data Well**: Random Forest can handle missing data without requiring imputation, which simplifies preprocessing.\r\n",
    "\r\n",
    "- **Feature Importance**: It provides feature importance scores, helping to understand which features contribute the most to the predictions.\r\n",
    "\r\n",
    "- **Versatility**: Random Forest can be used for both classification and regressioor each prediction.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcad8b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T08:52:26.665273Z",
     "iopub.status.busy": "2024-12-11T08:52:26.664512Z",
     "iopub.status.idle": "2024-12-11T09:10:25.392209Z",
     "shell.execute_reply": "2024-12-11T09:10:25.390802Z"
    },
    "papermill": {
     "duration": 1078.744829,
     "end_time": "2024-12-11T09:10:25.394523",
     "exception": false,
     "start_time": "2024-12-11T08:52:26.649694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20, 'class_weight': 'balanced', 'bootstrap': True}\n",
      "Best F1 Macro Score (CV): 0.77\n",
      "Training Accuracy: 0.91\n",
      "Test Accuracy: 0.86\n",
      "Training F1 Macro Score: 0.85\n",
      "Test F1 Macro Score: 0.77\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.87      0.91      6645\n",
      "         yes       0.52      0.77      0.62      1198\n",
      "\n",
      "    accuracy                           0.86      7843\n",
      "   macro avg       0.74      0.82      0.77      7843\n",
      "weighted avg       0.89      0.86      0.87      7843\n",
      "\n",
      "Confusion Matrix:\n",
      " [[5798  847]\n",
      " [ 281  917]]\n",
      "Submission file created successfully: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "# Here, we are specifying a range of hyperparameters to tune the Random Forest model.\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],  # The number of trees in the forest.\n",
    "    'max_features': ['sqrt', 'log2', None],  # Maximum number of features to consider when splitting a node.\n",
    "    'max_depth': [10, 20, 30, None],  # The maximum depth of each tree. 'None' means no limit.\n",
    "    'min_samples_split': [2, 5, 10],  # The minimum number of samples required to split an internal node.\n",
    "    'min_samples_leaf': [1, 2, 4],  # The minimum number of samples required to be at a leaf node.\n",
    "    'bootstrap': [True, False],  # Whether to use bootstrap samples when building trees.\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'],  # Class weighting to handle imbalanced classes.\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier model\n",
    "# We set a fixed random state for reproducibility of the results.\n",
    "random_f_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "# This is used to perform hyperparameter tuning by randomly sampling combinations from the parameter grid.\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=random_f_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of different parameter combinations to try.\n",
    "    scoring='f1_macro',  # Optimize based on the F1 macro score, which treats all classes equally.\n",
    "    cv=5,  # Perform 5-fold cross-validation to evaluate the model.\n",
    "    verbose=0,  # Show detailed output during the search.\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available CPU cores to speed up the computation.\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "# This step will search for the best combination of hyperparameters based on cross-validation performance.\n",
    "random_search.fit(x_train_transformed, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the best F1 score from cross-validation.\n",
    "print(f\"Best Parameters: {random_search.best_params_}\")\n",
    "print(f\"Best F1 Macro Score (CV): {random_search.best_score_:.2f}\")\n",
    "\n",
    "# Now, we use the best model found to make predictions on both the training and test sets.\n",
    "random_f_model = random_search.best_estimator_\n",
    "y_test_pred = random_f_model.predict(x_test_transformed)\n",
    "y_train_pred = random_f_model.predict(x_train_transformed)\n",
    "\n",
    "# Calculate and display the accuracy for both training and test sets.\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate and display the F1 macro score for both training and test sets.\n",
    "f1_macro_train = f1_score(y_train, y_train_pred, average='macro')\n",
    "f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Print out the accuracy and F1 scores.\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Training F1 Macro Score: {f1_macro_train:.2f}\")\n",
    "print(f\"Test F1 Macro Score: {f1_macro_test:.2f}\")\n",
    "\n",
    "# Print the classification report and confusion matrix for the test set to further evaluate model performance.\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# --- Submission Part ---\n",
    "# Predict on the test set (for submission)\n",
    "y_test_pred_submission = random_f_model.predict(z_test_transformed) \n",
    "\n",
    "# Create a DataFrame with 'id' and the predicted target values\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(0, len(z_test_transformed)),\n",
    "    'target': y_test_pred_submission\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf1b18",
   "metadata": {
    "papermill": {
     "duration": 0.008875,
     "end_time": "2024-12-11T09:10:25.412549",
     "exception": false,
     "start_time": "2024-12-11T09:10:25.403674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Why not Random Forest?\n",
    "\n",
    "- **Computationally Expensive**: Training many trees can be computationally expensive, especially with large datasets, and may require considerable memory.\n",
    "\n",
    "- **Difficult to Interpret**: While individual decision trees are interpretable, Random Forest models, due to their ensemble nature, are difficult to interpret.\n",
    "\n",
    "- **Slower Predictions**: For real-time applications, the ensemble of trees can lead to slower prediction times, as multiple trees must be queried for each prediction.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c6742",
   "metadata": {
    "papermill": {
     "duration": 0.00845,
     "end_time": "2024-12-11T09:10:25.429834",
     "exception": false,
     "start_time": "2024-12-11T09:10:25.421384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 42px; font-family: 'Montserrat', sans-serif; font-weight: 900; color: #7D7D7D; text-align: center; text-transform: uppercase; letter-spacing: 2px; background: linear-gradient(to right, #D3D3D3, #A9A9A9); color: white; padding: 15px 30px; border-radius: 25px; box-shadow: 0 10px 20px rgba(0, 0, 0, 0.2);\">\r\n",
    "    Model 4 - Decision Tree \r\n",
    "</p>\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cfe85",
   "metadata": {
    "papermill": {
     "duration": 0.008691,
     "end_time": "2024-12-11T09:10:25.447332",
     "exception": false,
     "start_time": "2024-12-11T09:10:25.438641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Why Decision Tree?\r\n",
    "\r\n",
    "- **Simple to Understand and Interpret**: Decision trees are easy to visualize and interpret, making them great for explaining model decisions to non-technical stakeholders.\r\n",
    "\r\n",
    "- **No Need for Feature Scaling**: Unlike other models, decision trees don’t require scaling of features, which simplifies preprocessing.\r\n",
    "\r\n",
    "- **Handles Non-linear Data**: Decision trees can capture non-linear relationships between features, making them flexible for various data types.\r\n",
    "\r\n",
    "- **Efficient on Large Datasets**: They are relatively efficient in handling large datasets and can handle both numerical and categorical data.\r\n",
    "\r\n",
    "- **Can Handle Missing Values**: Decision trees can handle missing data by splitting nodes on available data and leaving out missing to ensemble methods.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "279b1b84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T09:10:25.467172Z",
     "iopub.status.busy": "2024-12-11T09:10:25.466385Z",
     "iopub.status.idle": "2024-12-11T09:12:26.422963Z",
     "shell.execute_reply": "2024-12-11T09:12:26.421834Z"
    },
    "papermill": {
     "duration": 120.969209,
     "end_time": "2024-12-11T09:12:26.425466",
     "exception": false,
     "start_time": "2024-12-11T09:10:25.456257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Decision Tree: {'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Decision Tree Training Accuracy: 0.9010456516194848\n",
      "Decision Tree Test Accuracy: 0.8520974117047049\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.90      0.93      0.91      6645\n",
      "         yes       0.52      0.44      0.48      1198\n",
      "\n",
      "    accuracy                           0.85      7843\n",
      "   macro avg       0.71      0.68      0.70      7843\n",
      "weighted avg       0.84      0.85      0.85      7843\n",
      "\n",
      "Confusion Matrix (Test Data):\n",
      "[[6151  494]\n",
      " [ 666  532]]\n",
      "Decision Tree ROC-AUC Score: 0.8433854518001536\n",
      "Feature Importances:\n",
      "                   Feature  Importance\n",
      "2            num__duration    0.499196\n",
      "4               num__pdays    0.122485\n",
      "7       num__contact_month    0.066238\n",
      "22        cat__housing_yes    0.065947\n",
      "0                 num__age    0.064495\n",
      "8         num__contact_day    0.052897\n",
      "1             num__balance    0.048009\n",
      "5            num__previous    0.027041\n",
      "3            num__campaign    0.015926\n",
      "19    cat__marital_married    0.007049\n",
      "25          edu__education    0.007046\n",
      "6        num__contact_year    0.006229\n",
      "9     cat__job_blue-collar    0.002842\n",
      "12     cat__job_management    0.002041\n",
      "11      cat__job_housemaid    0.001894\n",
      "14  cat__job_self-employed    0.001670\n",
      "20     cat__marital_single    0.001513\n",
      "24  cat__contact_telephone    0.001326\n",
      "17     cat__job_technician    0.001308\n",
      "13        cat__job_retired    0.001167\n",
      "23           cat__loan_yes    0.001123\n",
      "15       cat__job_services    0.000928\n",
      "16        cat__job_student    0.000842\n",
      "21        cat__default_yes    0.000788\n",
      "10   cat__job_entrepreneur    0.000000\n",
      "18     cat__job_unemployed    0.000000\n",
      "Submission file created successfully: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Evaluate different impurity measures\n",
    "    'max_depth': [5, 10, 20, None],  # Depth of the tree, controlling the tree's complexity\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 5],  # Minimum samples required to be at a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Strategies for feature selection (None means using all features)\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "grid_search_dt = GridSearchCV(\n",
    "    dt_model,\n",
    "    param_grid_dt,\n",
    "    scoring='f1_macro',  # F1 macro is used to optimize for balanced precision and recall\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=0,  # Show progress messages during the fitting process\n",
    "    n_jobs=-1  # Use all available cores to speed up the computation\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search_dt.fit(x_train_transformed, y_train)\n",
    "\n",
    "# Get the best model from GridSearchCV\n",
    "dt_model = grid_search_dt.best_estimator_\n",
    "\n",
    "# Output the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters for Decision Tree:\", grid_search_dt.best_params_)\n",
    "\n",
    "# Evaluate the model on training data and test data\n",
    "y_train_pred = dt_model.predict(x_train_transformed)\n",
    "y_test_pred = dt_model.predict(x_test_transformed)\n",
    "\n",
    "# Training and Test Accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Display training and test accuracy\n",
    "print(\"Decision Tree Training Accuracy:\", train_accuracy)\n",
    "print(\"Decision Tree Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Display classification report (precision, recall, f1-score, support)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Confusion Matrix for test data\n",
    "print(\"Confusion Matrix (Test Data):\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# ROC-AUC Score: The area under the receiver operating characteristic curve\n",
    "roc_auc = roc_auc_score(y_test, dt_model.predict_proba(x_test_transformed)[:, 1])\n",
    "print(\"Decision Tree ROC-AUC Score:\", roc_auc)\n",
    "\n",
    "# Handle feature names for Feature Importances (if available)\n",
    "if hasattr(x_train_transformed, \"columns\"):  # Check if x_train_transformed is a DataFrame\n",
    "    feature_names = x_train_transformed.columns\n",
    "elif hasattr(preprocessor, \"get_feature_names_out\"):  # If using a preprocessor pipeline\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "else:\n",
    "    feature_names = [f\"Feature_{i}\" for i in range(x_train_transformed.shape[1])]\n",
    "\n",
    "# Create a DataFrame for Feature Importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importances sorted by importance\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Optionally, save feature importances to a CSV file\n",
    "feature_importances.to_csv('feature_importances.csv', index=False)\n",
    "\n",
    "# Make predictions for submission\n",
    "z_test_pred = dt_model.predict(z_test_transformed) \n",
    "\n",
    "# Create a submission DataFrame with predicted targets\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(z_test_pred)),\n",
    "    'target': z_test_pred  # Predictions for the test set\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# the submission file has been successfully created\n",
    "print(\"Submission file created successfully: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624833b",
   "metadata": {
    "papermill": {
     "duration": 0.008843,
     "end_time": "2024-12-11T09:12:26.443686",
     "exception": false,
     "start_time": "2024-12-11T09:12:26.434843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Not Why Decision Tree?\n",
    "\n",
    "- **Prone to Overfitting**: Decision trees can easily overfit if not pruned properly, especially when the tree is very deep.\n",
    "\n",
    "- **Instability**: Small variations in data can lead to very different trees, making decision trees less stable compared to ensemble methods like Random Forest.\n",
    "\n",
    "- **Bias Toward Features with More Categories**: Decision trees tend to favor features with more categories, which can lead to bias in the splits.\n",
    "\n",
    "- **Limited Performance on Complex Data**: While decision trees are flexible, they may struggle with complex datasets and fail to capture intricate patterns compared to ensemble methods.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d4df3",
   "metadata": {
    "papermill": {
     "duration": 0.008766,
     "end_time": "2024-12-11T09:12:26.461656",
     "exception": false,
     "start_time": "2024-12-11T09:12:26.452890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "----\n",
    "<p style=\"font-size: 36px; font-family: 'High Tower Text', serif; font-weight: bold; color: #333333; text-align: center; text-transform: uppercase;\">\r\n",
    "   THANK YOU\r\n",
    "</pp>\r\n",
    "\r\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9578279,
     "sourceId": 85062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1260.180416,
   "end_time": "2024-12-11T09:12:29.091491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-11T08:51:28.911075",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
